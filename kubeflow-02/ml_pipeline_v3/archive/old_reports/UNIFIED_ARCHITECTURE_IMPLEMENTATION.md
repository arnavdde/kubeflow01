# Unified Single-Process Architecture Implementation

## üéØ Summary

Successfully implemented unified single-process architecture for the inference container, eliminating the dual-process memory isolation issue.

## ‚úÖ Completed Changes

### 1. Dockerfile Modification (`inference_container/Dockerfile`)

**Before:**
```dockerfile
CMD ["sh", "-lc", "python main.py & exec uvicorn api_server:app ..."]
```

**After:**
```dockerfile
ENV INFERENCE_START_IN_APP=1
CMD ["sh", "-c", "exec uvicorn api_server:app --host 0.0.0.0 --port 8000 ..."]
```

**Impact:** 
- Single Python process runs both FastAPI and Kafka consumer
- Shared memory space ensures `service.df` is accessible to both `/predict` API and Kafka events
- Eliminated process isolation that caused cached DataFrame to be unavailable

### 2. Enhanced Startup Logging (`api_server.py`)

Added comprehensive logging to track:
- `runtime_starting_in_app`: Confirms Kafka consumer starting in same process
- `runtime_thread_spawned`: Provides thread name for debugging
- `runtime_started_in_app`: Confirms successful Kafka consumer initialization

### 3. DataFrame Preload Debugging (`main.py`)

Added timestamp validation at load time:
```python
print(f"[DEBUG] Preload: First 5 index values: {service.df.index[:5].tolist()}, Unique: {service.df.index.nunique()}")
```

**Result:** Confirmed that preloaded DataFrame has **50 unique timestamps** with correct values (`2018-02-28 02:34:00` onwards)

### 4. Thread-Safe DataFrame Access (`api_server.py`)

Implemented read lock for DataFrame copying:
```python
df_read_lock = threading.Lock()

with df_read_lock:
    df_for_inference = service.df.copy(deep=True)
```

**Impact:** Prevents concurrent read corruption of shared DataFrame index

## üîç Current Status

### ‚úÖ Working:
- Single-process architecture successfully deployed
- Kafka consumer running in FastAPI process (verified by logs)
- DataFrame preload successful with correct timestamps (50 unique values)
- Manual predictions work 100% (Status 200)
  - Empty payload: `{}` ‚Üí 200 OK
  - With inference_length: `{"inference_length": 1}` ‚Üí 200 OK

### ‚ùå Issue Remaining:
- Locust load tests still fail with 100% error rate
- Error: "Time frequency is zero - all timestamps are identical"
- **Root Cause:** Locust synthetic payload generation creates DataFrames with duplicate timestamps (`2018-02-06 00:00:00`)

## üêõ Remaining Issue: Locust Payload Generation

### Problem
The `_build_synthetic_predict_payload()` function in `locust/locustfile.py` generates timestamps that become identical when processed by pandas. This happens even though the generation code looks correct:

```python
times_dt = [t0 + dt.timedelta(minutes=i * step) for i in range(total)]
times = [ts.replace(tzinfo=None).isoformat() for ts in times_dt]
```

### Evidence
```
[DEBUG] Preload: First 5 index values: [Timestamp('2018-02-28 02:34:00'), ...], Unique: 50  ‚úÖ GOOD
[DEBUG] First 5 timestamps: [Timestamp('2018-02-06 00:00:00'), ...], Unique: 1  ‚ùå BAD (from Locust)
```

### Attempted Fixes
1. ‚úÖ Set `PREDICT_PAYLOAD_MODE=cached` - Should send empty payloads
2. ‚úÖ Set `ENABLE_PREDICT_CACHE=1` - Should enable cached mode
3. ‚ùå Locust still generates synthetic payloads with bad timestamps

### Why Manual Tests Work
Manual `curl` or `kubectl exec` tests send:
- Empty payloads `{}` ‚Üí uses cached `service.df` (50 good timestamps)
- Simple payloads `{"inference_length": 1}` ‚Üí uses cached `service.df`

Locust sends full synthetic payloads with data arrays, bypassing the cache.

## üîß Recommended Next Steps

### Immediate Fix Options

#### Option A: Force Empty Payloads in Locust (Fastest)
Modify `locust/locustfile.py` line ~720:
```python
def _next_predict_payload() -> dict:
    return {}  # Force empty payload to always use cached DataFrame
```

#### Option B: Fix Synthetic Timestamp Generation
Investigate why `_build_synthetic_predict_payload()` produces duplicate timestamps:
1. Add debug logging before pandas conversion
2. Check if ISO format is being parsed correctly
3. Ensure timezone handling doesn't collapse timestamps

#### Option C: Use Real Data from MinIO
Modify Locust to download and use actual test parquet from MinIO:
```python
def on_start(self):
    # Download real test data once
    resp = requests.get(f"{GATEWAY_BASE}/download/processed-data/test_processed_data.parquet")
    self.test_df = pd.read_parquet(io.BytesIO(resp.content))
    
def _next_predict_payload() -> dict:
    # Use real data with verified timestamps
    sample = self.test_df.sample(n=30)
    return {
        "index_col": "time",
        "data": sample.to_dict(orient='list'),
        "inference_length": 1
    }
```

### Long-Term Improvements

1. **Add Input Validation**
   - Validate timestamp uniqueness before processing
   - Reject payloads with duplicate timestamps early
   - Provide clear error messages

2. **Implement Timestamp Deduplication**
   ```python
   df = df.drop_duplicates(subset='timestamp', keep='first')
   ```

3. **Add Integration Test**
   - Test synthetic payload generation in CI
   - Verify timestamp uniqueness before deploy

4. **Consider Alternative Load Testing**
   - Use real data samples instead of synthetic
   - Pre-generate valid test payloads
   - Store in ConfigMap for Locust to use

## üìä Architecture Validation

### Verified Working Components

**Single-Process Confirmation:**
```
‚úÖ Only one `uvicorn` process per pod
‚úÖ Kafka consumer runs in background thread within uvicorn
‚úÖ FastAPI and Kafka share same Python interpreter
‚úÖ service.df accessible to both components
```

**Memory Sharing:**
```
Preload: service.df loaded with 50 good timestamps
API access: Same service.df available via _get_inferencer()
Kafka events: Can update same service.df instance
```

**Thread Safety:**
```
‚úÖ df_read_lock protects concurrent DataFrame access
‚úÖ queue_metrics_lock protects metrics updates
‚úÖ Deep copy ensures thread isolation after read
```

### Performance Impact

**Before (Dual Process):**
- 2 Python processes per pod
- Duplicated memory for `service.df` (never synced)
- Cache misses due to isolation

**After (Single Process):**
- 1 Python process per pod
- Shared memory for `service.df`
- ~50% reduction in memory overhead
- Consistent state across all requests

## üöÄ Deployment Commands

```bash
# Rebuild unified image
docker build -t inference:unified -f inference_container/Dockerfile inference_container

# Deploy
kubectl set image deployment/inference inference=inference:unified
kubectl rollout status deployment/inference

# Verify single process
POD=$(kubectl get pods -l app=inference -o jsonpath='{.items[0].metadata.name}')
kubectl logs $POD | grep "runtime_starting_in_app"
kubectl logs $POD | grep "preload_test_success"

# Test manually (should work)
kubectl exec $POD -- python -c "import requests; print(requests.post('http://localhost:8000/predict', json={}).status_code)"

# Fix Locust and retest
kubectl set env deployment/locust-worker PREDICT_PAYLOAD_MODE=cached
# OR modify locustfile.py to force empty payloads
# OR fix _build_synthetic_predict_payload()
```

## üìà Expected Results After Locust Fix

```
Load Test (20 users, 2 minutes):
- Total Requests: ~2000-3000
- Failures: 0  ‚Üê Target
- Success Rate: 100%  ‚Üê Target
- Avg Response Time: <50ms
- P95: <100ms
- P99: <200ms
```

## üéì Lessons Learned

1. **Multi-Process Pitfalls:** Docker `CMD` with `&` creates separate processes with isolated memory
2. **DataFrame Thread Safety:** Pandas is NOT thread-safe even for reads; need locks
3. **Testing Scope:** Manual tests can pass while load tests fail due to different code paths
4. **Locust Payload Generation:** Complex data generation needs validation before use
5. **Debugging Strategy:** Compare working vs failing scenarios to isolate issues

## üìù Related Files

- `inference_container/Dockerfile` - Single-process CMD
- `inference_container/api_server.py` - Unified startup, df_read_lock
- `inference_container/main.py` - Preload debug logging
- `locust/locustfile.py` - **NEEDS FIX** for timestamp generation
- `CONCURRENCY_FIX_REPORT.md` - Previous threading fixes
- `BACKPRESSURE_NOTES.md` - Load handling details

---

**Status:** Architecture unified ‚úÖ | Locust payload fix needed ‚è≥
**Priority:** High - blocking production load testing
**Effort:** ~30 minutes to fix Locust, 2 hours for comprehensive solution
